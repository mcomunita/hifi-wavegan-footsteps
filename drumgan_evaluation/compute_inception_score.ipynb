{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils.utils as utils\n",
    "import evaluation.inception_network as inception_network\n",
    "import evaluation.metrics.inception_score as inception_score\n",
    "import data.preprocessing as preprocessing\n",
    "import data.audio_transforms as audio_transforms\n",
    "import data.loaders as loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"footsteps_inception_model_best_2021-09-26.pt\",\n",
    "    \"comments\": \"inception trained on footsteps dataset\",\n",
    "    # \"state_dict_path\": \"/homes/mc309/hifi-wavegan/drumgan_evaluation/evaluation/inception_models/footsteps_inception_model_best_2021-09-26.pt\",\n",
    "    \"state_dict_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/drumgan_evaluation/evaluation/inception_models/footsteps_inception_model_best_2021-09-26.pt\",\n",
    "\n",
    "    # real samples used to train inception model\n",
    "    # \"real_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/_footsteps_data/zapsplat_misc_shoes_misc_surfaces_inception_network/\",\n",
    "    # real samples used to train gan\n",
    "    # \"real_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/_footsteps_data/zapsplat_pack_footsteps_high_heels_1s_aligned_for_inception_score/\",\n",
    "    # gan synthesised samples\n",
    "    # \"synth_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_13h23m-hifi/120k_generated_audio_large_for_is_and_kid/\",\n",
    "    # \"synth_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_19h46m-wave/120k_generated_audio_large_for_is_and_kid/\",\n",
    "    \n",
    "    \"output_path\": \"evaluation\",\n",
    "    \"output_folder\": \"evaluation_metrics\",\n",
    "    \n",
    "    \"batch_size\": 20,\n",
    "\n",
    "    \"samples_loader_config\": {\n",
    "        \"dbname\": \"footsteps\",\n",
    "        # \"data_path\": \"/homes/mc309/ccwavegan-hifigan-fresh/checkpoints/2021-09-20_19h46m-wave/120k_generated_audio_large_for_is/\",\n",
    "        \"data_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_19h46m-wave/120k_generated_audio_large_for_is/\",\n",
    "        \"criteria\": {},\n",
    "        \"shuffle\": True,\n",
    "        \"tr_val_split\": 1.0\n",
    "    },\n",
    "\n",
    "    \"transform_config\": {\n",
    "        \"transform\": \"stft\",\n",
    "        \"fade_out\": True,\n",
    "        \"fft_size\": 1024,\n",
    "        \"win_size\": 1024,\n",
    "        \"n_frames\": 64,\n",
    "        \"hop_size\": 256,\n",
    "        \"log\": False,\n",
    "        \"ifreq\": False,\n",
    "        \"sample_rate\": 16000,\n",
    "        \"audio_length\": 8192\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "state_dict_path = config['state_dict_path']\n",
    "output_path = utils.mkdir_in_path(config['output_path'], config['output_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataloader and processor for real samples\n",
    "\n",
    "samples_loader_config = config['samples_loader_config']\n",
    "\n",
    "transform_config = config['transform_config']\n",
    "transform = transform_config['transform']\n",
    "\n",
    "dbname = samples_loader_config['dbname']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "processor = preprocessing.AudioProcessor(**transform_config)\n",
    "\n",
    "loader_module = loaders.get_data_loader(dbname)\n",
    "\n",
    "samples_loader = loader_module(name=dbname + '_' + transform, preprocessing=processor, **samples_loader_config)\n",
    "\n",
    "n_samples = len(samples_loader)\n",
    "print('n_samples: ', n_samples)\n",
    "\n",
    "samples_data_loader = DataLoader(samples_loader,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "\n",
    "\n",
    "# load inception model\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "inception_footsteps = inception_network.SpectrogramInception3(state_dict['fc.weight'].shape[0], aux_logits=False)\n",
    "inception_footsteps.load_state_dict(state_dict)\n",
    "# inception_footsteps = inception_footsteps.to(device)\n",
    "\n",
    "# inception model is trained on mel spectrograms\n",
    "mel = audio_transforms.MelScale(sample_rate=transform_config['sample_rate'],\n",
    "                fft_size=transform_config['fft_size'],\n",
    "                n_mel=transform_config.get('n_mel', 256),\n",
    "                rm_dc=True)\n",
    "# mel = mel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute IS\n",
    "n_iter = 1\n",
    "is_maker_samples = inception_score.InceptionScore()\n",
    "inception_score_samples = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print(\"iter: \", i)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(samples_data_loader):\n",
    "            input, labels = data\n",
    "            # input.to(device)\n",
    "            input = mel(input.float())\n",
    "            # input.to(device)\n",
    "            mag_input = F.interpolate(input[:, 0:1], (299, 299))\n",
    "            # mag_input.to(device)\n",
    "            \n",
    "            preds = inception_footsteps(mag_input.float())\n",
    "            \n",
    "            is_maker_samples.updateWithMiniBatch(preds)\n",
    "            inception_score_samples.append(is_maker_samples.getScore())\n",
    "            \n",
    "            print('batch: ', batch_idx, 'IS: ', is_maker_samples.getScore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "IS_mean = np.mean(inception_score_samples)\n",
    "IS_std = np.std(inception_score_samples)\n",
    "output_file = f'{output_path}/IS_{str(n_samples)}_{model_name}_{datetime.now().strftime(\"%d-%m-%y_%H_%M\")}.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(str(IS_mean) + '\\n')\n",
    "    f.write(str(IS_std))\n",
    "    f.close()\n",
    "\n",
    "print(\"IS_mean: \", IS_mean)\n",
    "print(\"IS_std: \", IS_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0f12b0897b6aac95a4ccbfd50bf012831105a0b659673d27dfe08e7727297b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
