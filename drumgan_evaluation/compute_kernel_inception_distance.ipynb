{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import utils.utils as utils\n",
    "import evaluation.inception_network as inception_network\n",
    "import evaluation.metrics.maximum_mean_discrepancy as maximum_mean_discrepancy\n",
    "import evaluation.metrics.mmd as mmd_function\n",
    "import data.preprocessing as preprocessing\n",
    "import data.audio_transforms as audio_transforms\n",
    "import data.loaders as loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "config = {\n",
    "    \"model_name\": \"footsteps_inception_model_best_2021-09-26.pt\",\n",
    "    \"comments\": \"inception trained on footsteps dataset\",\n",
    "    # \"state_dict_path\": \"/homes/mc309/hifi-wavegan/drumgan_evaluation/evaluation/inception_models/footsteps_inception_model_best_2021-09-26.pt\",\n",
    "    \"state_dict_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/drumgan_evaluation/evaluation/inception_models/footsteps_inception_model_best_2021-09-26.pt\",\n",
    "    \n",
    "    # real samples used to train inception model\n",
    "    # \"real_samples_path\": \"/homes/mc309/_data/zapsplat_misc_shoes_misc_surfaces_inception_network/\",\n",
    "    # real samples used to train gan\n",
    "    # \"real_samples_path\": \"/homes/mc309/_data/zapsplat_pack_footsteps_high_heels_1s_aligned_for_inception_score/\",\n",
    "    # gan synthesised samples\n",
    "    # \"synth_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_13h23m-hifi/120k_generated_audio_large_for_is_and_kid/\",\n",
    "    # \"synth_samples_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_19h46m-wave/120k_generated_audio_large_for_is_and_kid/\",\n",
    "    \n",
    "    \"output_path\": \"evaluation\",\n",
    "    \"output_folder\": \"evaluation_metrics\",\n",
    "    \n",
    "    \"batch_size\": 20,\n",
    "\n",
    "    \"real_samples_loader_config\": {\n",
    "        \"dbname\": \"footsteps\",\n",
    "        \"data_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_13h23m-hifi/120k_generated_audio_large_for_is_and_kid/\",\n",
    "        \"criteria\": {},\n",
    "        \"shuffle\": True,\n",
    "        \"tr_val_split\": 1.0\n",
    "    },\n",
    "\n",
    "    \"synth_samples_loader_config\": {\n",
    "        \"dbname\": \"footsteps\",\n",
    "        \"data_path\": \"/Users/Marco/Documents/OneDrive - Queen Mary, University of London/PHD/REPOS/hifi-wavegan/checkpoints/2021-09-20_19h46m-wave/120k_generated_audio_large_for_is_and_kid/\",\n",
    "        \"criteria\": {},\n",
    "        \"shuffle\": True,\n",
    "        \"tr_val_split\": 1.0\n",
    "    },\n",
    "    \n",
    "    \"transform_config\": {\n",
    "        \"transform\": \"stft\",\n",
    "        \"fade_out\": True,\n",
    "        \"fft_size\": 1024,\n",
    "        \"win_size\": 1024,\n",
    "        \"n_frames\": 64,\n",
    "        \"hop_size\": 256,\n",
    "        \"log\": False,\n",
    "        \"ifreq\": False,\n",
    "        \"sample_rate\": 16000,\n",
    "        \"audio_length\": 8192\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "state_dict_path = config['state_dict_path']\n",
    "output_path = utils.mkdir_in_path(config['output_path'], config['output_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataloader and processor for real and synthesised samples\n",
    "real_samples_loader_config = config['real_samples_loader_config']\n",
    "synth_samples_loader_config = config['synth_samples_loader_config']\n",
    "\n",
    "transform_config = config['transform_config']\n",
    "transform = transform_config['transform']\n",
    "\n",
    "dbname = real_samples_loader_config['dbname']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "processor = preprocessing.AudioProcessor(**transform_config)\n",
    "\n",
    "loader_module = loaders.get_data_loader(dbname)\n",
    "\n",
    "real_samples_loader = loader_module(name=dbname + '_' + transform, preprocessing=processor, **real_samples_loader_config)\n",
    "synth_samples_loader = loader_module(name=dbname + '_' + transform, preprocessing=processor, **synth_samples_loader_config)\n",
    "\n",
    "n_real_samples = len(real_samples_loader)\n",
    "print('n_real_samples: ', n_real_samples)\n",
    "n_synth_samples = len(synth_samples_loader)\n",
    "print('n_synth_samples: ', n_synth_samples)\n",
    "\n",
    "real_samples_data_loader = DataLoader(real_samples_loader,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "synth_samples_data_loader = DataLoader(synth_samples_loader,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "\n",
    "# load inception model\n",
    "device = 'cuda' if utils.GPU_is_available() else 'cpu'\n",
    "\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "inception_footsteps = inception_network.SpectrogramInception3(state_dict['fc.weight'].shape[0], aux_logits=False)\n",
    "inception_footsteps.load_state_dict(state_dict)\n",
    "# inception_footsteps = inception_footsteps.to(device)\n",
    "\n",
    "mel = audio_transforms.MelScale(sample_rate=transform_config['sample_rate'],\n",
    "                fft_size=transform_config['fft_size'],\n",
    "                n_mel=transform_config.get('n_mel', 256),\n",
    "                rm_dc=True)\n",
    "# mel = mel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings for real samples\n",
    "real_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(real_samples_data_loader):\n",
    "        input, labels = data\n",
    "        input.to(device)\n",
    "        input = mel(input.float())\n",
    "        mag_input = F.interpolate(input[:, 0:1], (299, 299))\n",
    "        \n",
    "        preds = inception_footsteps(mag_input.float())\n",
    "        \n",
    "        real_logits.append(preds)\n",
    "\n",
    "        print('batch: ', batch_idx)\n",
    "    \n",
    "real_logits = torch.cat(real_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings for synthesised samples\n",
    "synth_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(synth_samples_data_loader):\n",
    "        input, labels = data\n",
    "        input.to(device)\n",
    "        input = mel(input.float())\n",
    "        mag_input = F.interpolate(input[:, 0:1], (299, 299))\n",
    "        \n",
    "        preds = inception_footsteps(mag_input.float())\n",
    "        \n",
    "        synth_logits.append(preds)\n",
    "\n",
    "        print('batch: ', batch_idx)\n",
    "    \n",
    "synth_logits = torch.cat(synth_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute KID squared distance\n",
    "mmd_distance = maximum_mean_discrepancy.mmd(real_logits, synth_logits)\n",
    "print('kid squared mmd: ', float(mmd_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD A vs B\n",
    "n_mmds = 1000\n",
    "n_samples_A = real_logits.shape[0]\n",
    "n_samples_B = synth_logits.shape[0]\n",
    "n_samples_mmd = int(min(n_samples_A, n_samples_B) / 2)\n",
    "\n",
    "mmds = []\n",
    "\n",
    "for i in range(0, n_mmds):\n",
    "    print(i, end = ' ')\n",
    "    idx1 = sorted(random.sample(range(0, n_samples_A-1), n_samples_mmd))\n",
    "    idx2 = sorted(random.sample(range(0, n_samples_B-1), n_samples_mmd))\n",
    "    real_logits_subset = real_logits[idx1]\n",
    "    synth_logits_subset = synth_logits[idx2]\n",
    "    mmd = mmd_function.mmd(x=real_logits_subset, y=synth_logits_subset, distance='manhattan')\n",
    "    mmds.append(mmd)\n",
    "print()\n",
    "print('kid mean l1 mmd: ',np.average(mmds))\n",
    "print('kid std l1 mmd: ', np.std(mmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "output_file = f'{output_path}/KID_{model_name}_{datetime.now().strftime(\"%d-%m-%y_%H_%M\")}.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(str(mean_MMD)+'\\n')\n",
    "    f.write(str(var_MMD))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0f12b0897b6aac95a4ccbfd50bf012831105a0b659673d27dfe08e7727297b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
